{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_directory(directory):\n",
    "    files_list = []\n",
    "\n",
    "    for entry in os.listdir(directory):\n",
    "        full_path = os.path.join(directory, entry)\n",
    "\n",
    "        if os.path.isfile(full_path):\n",
    "            files_list.append(entry)\n",
    "\n",
    "    return files_list\n",
    "\n",
    "def extract_number(filename):\n",
    "    match = re.search(r'-?\\d+', filename)\n",
    "    return int(match.group()) if match else None\n",
    "\n",
    "def extract_non_number(filename):\n",
    "    match = re.search(r'_', filename)\n",
    "    return match.group() if match else None\n",
    "\n",
    "def find_opening_closing(files):\n",
    "    numbered_files = [(extract_number(file), file) for file in files if (extract_number(file)) != None]\n",
    "    \n",
    "    opening_file_tuple = min(numbered_files, key=lambda x: x[0])\n",
    "    closing_file_tuple = max(numbered_files, key=lambda x: x[0])\n",
    "    \n",
    "    numbered_files.remove(opening_file_tuple)\n",
    "    opening_file_1_tuple = min(numbered_files, key=lambda x: x[0])\n",
    "    \n",
    "    numbered_files.remove(opening_file_1_tuple)\n",
    "    opening_file_2_tuple = min(numbered_files, key=lambda x: x[0])\n",
    "    \n",
    "    opening_file = opening_file_tuple[1]\n",
    "    opening_file_1 = opening_file_1_tuple[1]\n",
    "    opening_file_2 = opening_file_2_tuple[1]\n",
    "    closing_file = closing_file_tuple[1]\n",
    "    \n",
    "    return opening_file_2, opening_file_1, opening_file, closing_file\n",
    "\n",
    "def find_body_struktur(files):\n",
    "    non_number_files = [(extract_non_number(file), file) for file in files if (extract_non_number(file)) != None]\n",
    "    non_number_files = non_number_files[0][1]\n",
    "    return non_number_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    file_path = file_path.strip()\n",
    "    if file_path == '':\n",
    "        return\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "def read_after_first_blank_line(file_content):\n",
    "    content = []\n",
    "    blank_line_found = False\n",
    "\n",
    "    for line in file_content.splitlines():\n",
    "        if line.strip() == \"\":\n",
    "            blank_line_found = True\n",
    "            continue\n",
    "\n",
    "        if blank_line_found:\n",
    "            content.append(line)\n",
    "\n",
    "    return '\\n'.join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(part, new_df):\n",
    "    total_rows = len(new_df)\n",
    "    part_size = total_rows\n",
    "\n",
    "    for i in range(1):\n",
    "        print(f'Start Creating Dataset {part}...')\n",
    "\n",
    "        idx = new_df.iloc[i*part_size:(i+1)*part_size]\n",
    "        dataset = Dataset.from_pandas(idx)\n",
    "\n",
    "        print(f'Start Saving Dataset {part}...')\n",
    "        print(f'Saving at ../dataset-surface-info/{part}/{part}')\n",
    "        dataset.save_to_disk(f'../dataset-surface-info/{part}/{part}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_excluded():\n",
    "    file_name = 'amandemen.csv'\n",
    "    print(f'Start Reading Files {file_name}...')\n",
    "    df = pd.read_csv(file_name)\n",
    "    return df\n",
    "\n",
    "def process_df_excluded(df, col_name):\n",
    "    df['reg_id_lower'] = df[col_name].apply(lambda x: x.lower())\n",
    "    df['reg_id'] = df[col_name]\n",
    "    df = df[['reg_id', 'reg_id_lower']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df():\n",
    "    file_name = 'core/regulatory_map_surface_info.csv'\n",
    "    print(f'Start Reading Files {file_name}...')\n",
    "    df = pd.read_csv(file_name)\n",
    "    return df\n",
    "\n",
    "def process_df(df, excluded_df):\n",
    "    df['label'] = df['regulatory'].apply(lambda x: x.split('_')[0])\n",
    "    df['regulatory_lower'] = df['regulatory'].apply(lambda x: x.lower())\n",
    "    \n",
    "    value_counts = df['label'].value_counts()\n",
    "    df = df[~df['regulatory_lower'].isin(excluded_df['reg_id_lower'])]\n",
    "    \n",
    "    df = df[['regulatory', 'label', 'file_txt', 'file_ttl']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_separate_surface(part, df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    if part == 'opening':\n",
    "        idx = 3\n",
    "    elif part == 'closing':\n",
    "        idx = 4\n",
    "    elif part == 'body struktur':\n",
    "        idx = 5\n",
    "    \n",
    "    df['file_ttl'] = df['file_ttl'].apply(lambda x: x.replace('new_2_turtle_files', f'new_{idx}_turtle_files'))\n",
    "    df['triples'] = df['file_ttl'].apply(lambda x: read_after_first_blank_line(read_file(x)))\n",
    "    \n",
    "    df['folder_txt'] = df['file_txt'].apply(lambda x: x.replace('new_1_text_files', 'new_split_txt').split('.')[0])\n",
    "    df[['opening', 'opening_1', 'opening_2', 'closing']] = df['folder_txt'].apply(lambda x: pd.Series(find_opening_closing(list_files_in_directory(x))))\n",
    "    \n",
    "    if part == 'opening':\n",
    "        df['txt'] = df['opening']\n",
    "        df['txt_1'] = df['opening_1']\n",
    "        df['txt_2'] = df['opening_2']\n",
    "        \n",
    "        df['file_txt'] = df['folder_txt'] + '/' + df['txt']\n",
    "        df['file_txt_1'] = df['folder_txt'] + '/' + df['txt_1']\n",
    "        df['file_txt_2'] = df['folder_txt'] + '/' + df['txt_2']\n",
    "        \n",
    "        df['text'] = df['file_txt'].apply(lambda x: read_file(x).strip())\n",
    "        df['text_1'] = df['file_txt_1'].apply(lambda x: read_file(x).strip())\n",
    "        df['text_2'] = df['file_txt_2'].apply(lambda x: read_file(x).strip())\n",
    "        \n",
    "        new_df = df[['regulatory', 'label', 'text', 'text_1', 'text_2', 'triples']]\n",
    "        \n",
    "    elif part == 'closing':\n",
    "        df['txt'] = df['closing']\n",
    "        \n",
    "        df['file_txt'] = df['folder_txt'] + '/' + df['txt']\n",
    "        df['text'] = df['file_txt'].apply(lambda x: read_file(x).strip())\n",
    "\n",
    "        new_df = df[['regulatory', 'label', 'text', 'triples']]\n",
    "        \n",
    "    elif part == 'body struktur':\n",
    "        df['file_txt'] = df['folder_txt'] + '/' + '_.txt'\n",
    "        df['text'] = df['file_txt'].apply(lambda x: read_file(x).strip())\n",
    "        \n",
    "        df['file_txt_1'] = df['folder_txt'] + '/' + '*.txt'\n",
    "        df['text_1'] = df['file_txt_1'].apply(lambda x: read_file(x).strip())\n",
    "        \n",
    "        new_df = df[['regulatory', 'label', 'text', 'text_1', 'triples']]\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excluded = read_df_excluded()\n",
    "df_excluded = process_df_excluded(df_excluded, 'regulatory')\n",
    "df_excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_df()\n",
    "df = process_df(df, df_excluded)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = create_dataset_separate_surface('opening', df)\n",
    "new_df.reset_index(drop=True, inplace=True)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset('new-opening', new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_2 = create_dataset_separate_surface('closing', df)\n",
    "new_df_2.reset_index(drop=True, inplace=True)\n",
    "new_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset('new-closing', new_df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body struktur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_3 = create_dataset_separate_surface('body struktur', df)\n",
    "new_df_3.reset_index(drop=True, inplace=True)\n",
    "new_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset('new-body-struktur', new_df_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "opening_dataset_name = \"../dataset-surface-info/new-dataset/new-opening/new-opening\"\n",
    "opening_dataset = load_from_disk(opening_dataset_name)\n",
    "closing_dataset_name = \"../dataset-surface-info/new-dataset/new-closing/new-closing\"\n",
    "closing_dataset = load_from_disk(closing_dataset_name)\n",
    "body_st_dataset_name = \"../dataset-surface-info/new-body-struktur/new-body-struktur\"\n",
    "body_st_dataset = load_from_disk(body_st_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_stratify(dataset, stratify_by_column, init=None):\n",
    "    dataset = dataset.sort('regulatory')\n",
    "    if init == None:\n",
    "        dct = dataset.train_test_split(test_size=0.5, seed=42, stratify_by_column=stratify_by_column)\n",
    "    else:\n",
    "        dct = dataset.class_encode_column(stratify_by_column).train_test_split(test_size=0.5, seed=42, stratify_by_column=stratify_by_column)\n",
    "    train, test = dct['train'], dct['test']\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def create_chunk(dataset, part, stratify_by=None):\n",
    "    label_counts = Counter(dataset[stratify_by])\n",
    "    labels_to_keep = [label for label, count in label_counts.items() if count >= 20]\n",
    "    filtered_dataset = dataset.filter(lambda x: x[stratify_by] in labels_to_keep)\n",
    "    c_filtered_dataset = dataset.filter(lambda x: x[stratify_by] not in labels_to_keep)\n",
    "\n",
    "    a, b = split_stratify(filtered_dataset, stratify_by, True)\n",
    "    a1, a2 = split_stratify(a, stratify_by)\n",
    "    b1, b2 = split_stratify(b, stratify_by)\n",
    "    a11, a12 = split_stratify(a1, stratify_by)\n",
    "    a21, a22 = split_stratify(a2, stratify_by)\n",
    "    b11, b12 = split_stratify(b1, stratify_by)\n",
    "    b21, b22 = split_stratify(b2, stratify_by)\n",
    "\n",
    "    lst = [a11, a12, a21, a22, b11, b12, b21, b22, c_filtered_dataset]\n",
    "\n",
    "    for i in range(len(lst)):\n",
    "        process_dataset_chunk(part, lst[i], i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_chunk(part, dataset, idx):\n",
    "    print(f'Start Saving Dataset {part} {idx}...')\n",
    "    print(f'Saving at ../dataset-surface-info/{part}/{part}-{idx}')\n",
    "    dataset.save_to_disk(f'../dataset-surface-info/{part}/{part}-{idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_chunk(opening_dataset, 'new-new-opening', stratify_by='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_chunk(closing_dataset, 'new-new-closing', stratify_by='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_chunk(body_st_dataset, 'new-new-body-struktur', stratify_by='label')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
